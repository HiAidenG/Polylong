{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure your runtime has a gpu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Transformer-Explainability'...\n",
      "remote: Enumerating objects: 377, done.\u001b[K\n",
      "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
      "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
      "remote: Total 377 (delta 127), reused 74 (delta 74), pack-reused 225\u001b[K\n",
      "Receiving objects: 100% (377/377), 3.83 MiB | 3.83 MiB/s, done.\n",
      "Resolving deltas: 100% (190/190), done.\n",
      "warning: the following paths have collided (e.g. case-sensitive paths\n",
      "on a case-insensitive filesystem) and only one from the same\n",
      "colliding group is in the working tree:\n",
      "\n",
      "  'data/Imagenet.py'\n",
      "  'data/imagenet.py'\n",
      "Requirement already satisfied: Pillow>=8.1.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (8.3.2)\n",
      "Requirement already satisfied: einops==0.3.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: h5py==2.8.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: imageio==2.9.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: matplotlib==3.3.2 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: opencv_python==3.4.2.17 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (3.4.2.17)\n",
      "Requirement already satisfied: scikit_image==0.17.2 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.17.2)\n",
      "Requirement already satisfied: scipy==1.5.2 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: sklearn in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (0.0.post1)\n",
      "Requirement already satisfied: torch==1.7.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (1.7.0)\n",
      "Requirement already satisfied: torchvision==0.8.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (0.8.1)\n",
      "Requirement already satisfied: tqdm==4.51.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 12)) (4.51.0)\n",
      "Requirement already satisfied: transformers==3.5.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (3.5.1)\n",
      "Requirement already satisfied: utils==1.0.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 14)) (1.0.1)\n",
      "Requirement already satisfied: Pygments>=2.7.4 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from -r requirements.txt (line 15)) (2.13.0)\n",
      "Requirement already satisfied: six in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from h5py==2.8.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from h5py==2.8.0->-r requirements.txt (line 3)) (1.19.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 5)) (2021.5.30)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from scikit_image==0.17.2->-r requirements.txt (line 7)) (2020.9.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from scikit_image==0.17.2->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from scikit_image==0.17.2->-r requirements.txt (line 7)) (2.5.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from torch==1.7.0->-r requirements.txt (line 10)) (4.1.1)\n",
      "Requirement already satisfied: future in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from torch==1.7.0->-r requirements.txt (line 10)) (0.18.2)\n",
      "Requirement already satisfied: dataclasses in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from torch==1.7.0->-r requirements.txt (line 10)) (0.8)\n",
      "Requirement already satisfied: packaging in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (0.9.3)\n",
      "Requirement already satisfied: sacremoses in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (0.0.53)\n",
      "Requirement already satisfied: protobuf in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (3.19.6)\n",
      "Requirement already satisfied: filelock in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (3.0.4)\n",
      "Requirement already satisfied: requests in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (2.28.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from transformers==3.5.1->-r requirements.txt (line 13)) (0.1.91)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from networkx>=2.0->scikit_image==0.17.2->-r requirements.txt (line 7)) (4.4.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from requests->transformers==3.5.1->-r requirements.txt (line 13)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from requests->transformers==3.5.1->-r requirements.txt (line 13)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from requests->transformers==3.5.1->-r requirements.txt (line 13)) (1.26.11)\n",
      "Requirement already satisfied: click in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from sacremoses->transformers==3.5.1->-r requirements.txt (line 13)) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from sacremoses->transformers==3.5.1->-r requirements.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from click->sacremoses->transformers==3.5.1->-r requirements.txt (line 13)) (4.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from importlib-metadata->click->sacremoses->transformers==3.5.1->-r requirements.txt (line 13)) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: captum in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (0.5.0)\n",
      "Requirement already satisfied: matplotlib in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from captum) (3.3.2)\n",
      "Requirement already satisfied: numpy in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from captum) (1.19.5)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from captum) (1.7.0)\n",
      "Requirement already satisfied: dataclasses in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from torch>=1.6->captum) (0.8)\n",
      "Requirement already satisfied: future in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from torch>=1.6->captum) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from torch>=1.6->captum) (4.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib->captum) (2021.5.30)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib->captum) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib->captum) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/aidenh20/miniconda3/envs/transformer/lib/python3.6/site-packages (from gensim) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.kmers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c06cf5f29ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkMerDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkMer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkmers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.kmers'"
     ]
    }
   ],
   "source": [
    "# Clone transformer explainbility repo, install other dependencies\n",
    "!git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
    "\n",
    "import os;\n",
    "os.chdir(f'./Transformer-Explainability')\n",
    "  \n",
    "%pip install -r requirements.txt\n",
    "%pip install captum\n",
    "%pip install gensim\n",
    "\n",
    "from utils.kmers import kMerDF, kMer\n",
    "import utils.kmers as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy import stats\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.13k/1.13k [00:00<00:00, 485kB/s]\n",
      "Downloading: 100%|██████████| 357M/357M [00:32<00:00, 10.9MB/s] \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dd39303844e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#download pre-trained model off of huggingface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AidenH20/DNABERT-500down'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'negative'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/transformer/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#download pre-trained model off of huggingface\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('AidenH20/DNABERT-500down', id2label={0: 'negative', 1: 'positive'}).to('cuda')\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('AidenH20/DNABERT-500down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "explanations = Generator(model)\n",
    "\n",
    "classifications = ['unstable', 'stable']\n",
    "\n",
    "def gen_explanation(text_batch, true_class):\n",
    "  \"\"\"This function is adapted from Chefer et al's. BERT-explainibility \n",
    "  notebook. Link to their github repo is here: \n",
    "  https://github.com/hila-chefer/Transformer-Explainability\n",
    "  \"\"\"\n",
    "  # encode a sentence\n",
    "  encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "  input_ids = encoding['input_ids'].to(\"cuda\")\n",
    "  attention_mask = encoding['attention_mask'].to(\"cuda\")\n",
    "  \n",
    "  # generate an explanation for the input\n",
    "  expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "\n",
    "  # get the model classification\n",
    "  output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
    "  classification = output.argmax(dim=-1).item()\n",
    "\n",
    "  tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "  return [(tokens[i], expl[i].item()) for i in range(len(tokens))]\n",
    "\n",
    "%cd /examples\n",
    "result = pd.DataFrame(pd.read_csv('XLTestingData.csv'))\n",
    "\n",
    "seqs = result[['b3', 'label']]\n",
    "\n",
    "%cd /512ds\n",
    "with open('512expl.csv', 'a') as f:\n",
    "  f.write('scores,' + 'label' + '\\n')\n",
    "i = 0\n",
    "while i < len(seqs):\n",
    "  with open('512expl.csv', 'a') as f:\n",
    "    expl = gen_explanation(text_batch = seqs['b3'][i], true_class = seqs['label'][i])\n",
    "    expl = [item[1] for item in expl[1:-1]]\n",
    "    expl = ' '.join(map(str, expl))\n",
    "    string = '{0},{1}\\n'.format(str(expl), str(seqs['label'][i]))\n",
    "    f.write(string)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aidenh20/Polylong/examples/512ds\n"
     ]
    }
   ],
   "source": [
    "# temporary\n",
    "%cd ../examples/512ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load attention scores, convert to numpy array\n",
    "expl_scores = pd.read_csv('512expl.csv')['scores']\n",
    "expl_scores = [str(i).split(' ') for i in expl_scores]\n",
    "expl_scores = np.array(expl_scores).astype(float)\n",
    "\n",
    "validation_labels = np.array(pd.read_csv('512expl.csv')['label'])\n",
    "\n",
    "#load DNA sequences and their labels, convert to numpy arrays.\n",
    "dnadf = pd.read_csv('../XLTestingData.csv')['b3']\n",
    "dnadf = np.array([str(i).split(' ') for i in dnadf])\n",
    "labels = pd.read_csv('../XLTestingData.csv')['label'].to_numpy()\n",
    "\n",
    "\n",
    "#make sure all the sequences in both dataframes are sorted in the same order\n",
    "assert validation_labels.all() == labels.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kMerDF[0][506]: CAGGCG\n",
      "dnadf[0][506]: CAGGCG\n"
     ]
    }
   ],
   "source": [
    "#create the kMerDF object\n",
    "X = kMerDF(expl_scores, dnadf, labels)\n",
    "\n",
    "#get the kMer object at row 0, column 506\n",
    "print('kMerDF[0][506]: ' + X[0][506].seq)\n",
    "\n",
    "#showing this is the same kmer as the first in the dnadf\n",
    "print('dnadf[0][506]: ' + dnadf[0][506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers_matching_region(kmer, kmerdf, start, end):\n",
    "    \"\"\"Returns a list of kMers that match the region of the kMer passed in.\n",
    "    \"\"\"\n",
    "    matching_kmers = []\n",
    "    for i in range(start, end):\n",
    "        if kmerdf[i].seq == kmer.seq:\n",
    "            matching_kmers.append(kmerdf[i])\n",
    "    return matching_kmers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('contig_kmers.fa', 'a') as f:\n",
    "    for l in X.get_contiguous_kmers(zscore = 1): #get contiguous regions with z-score >= 1.\n",
    "        if l != [] and l[0].pos <= 100: #TODO: weird behavior of get_contiguous_kmers with the empty lists\n",
    "            # write a unique sequence header\n",
    "            f.write('>ID' + str(l[0].ID) + 'label' + str(l[0].label) + ':' + str(l[0].pos) + '\\n')\n",
    "            r_seq = ''\n",
    "            for kmer in l: #l is a list of kMer objects\n",
    "                r_seq += kmer.seq + ' '\n",
    "            f.write(utils.kmer2seq(r_seq.strip()) + '\\n')\n",
    "\n",
    "# TODO: check the other bins, so positions 100-200, 200-300, 300-400, 400-507. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I plug in the results from the previous cell into DREME, using positive sequences as a query and negative as the control."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 ('transformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83b8464dbb052d81911aaaacd0dd908c7d09515d53271f159ad22fa642d6bb62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
